{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:What is the definition of a target function? In the sense of a real-life example, express the target\n",
    "function. How is a target function&#39;s fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target function is essentially the formula that an algorithm feeds data to in order to calculate predictions. As in algebra, it is common when training AI to find the variable from the solution, working in reverse. The function as defined by f is applied to the input (I) to produce the output (I), Therefore O= f(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitness is assessed based on various evaluation metrics for classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:What are predictive models, and how do they work? What are descriptive types, and how do you\n",
    "use them? Examples of both types of models should be provided. Distinguish between these two\n",
    "forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Analytics will give you a vision into the past and tells you: what has happened? Whereas the Predictive Analytics will recognize the future and tells you: What might happen in future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A descriptive model will exploit the past data that are stored in databases and provide you with the accurate report. In a Predictive model, it identifies patterns found in past and transactional data to find risks and future outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A king hired a data scientist to find animals in the forest for hunting. The data scientist has access to data warehouse, which has information about the forest, its habitat and what is happening in the forest.\n",
    "\n",
    "On day one, the data scientist offered the king with a report showing where he found the highest number of animals in the forest in past one year. This report helped the king to take a decision on where he can find more animals for hunting. This is an example of Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next day the data scientist identifies the possibility of finding the particular animal at specific places and time using innovative tools. This is an example of Predictive Analysis. This helps the king to find the animals easily with minimal efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various\n",
    "measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "Yes! You got that right, we want our model to focus on True positive and True Negative. Accuracy is one metric which gives the fraction of predictions our model got right. Formally, accuracy has the following definition:\n",
    "\n",
    "Accuracy = Number of correct predictions / Total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall gives the fraction you correctly identified as positive out of all positives.\n",
    "\n",
    "Recall = True Positives / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision gives the fraction of correctly identified as positive out of all predicted as positives\n",
    "\n",
    "Precision = True Positive / (True Positives + False Positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score\n",
    "It is defined as the harmonic mean of the model’s precision and recall\n",
    "F1 = (Precison * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:i)In the sense of machine learning models, what is underfitting? What is the most common\n",
    "reason for underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) What does it mean to overfit? When is it going to happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to avoid overfitting is to use a lot of data. The main reason overfitting happens is because you have a small dataset and you try to learn from it. The algorithm will have greater control over this small dataset and it will make sure it satisfies all the datapoints exactly. But if you have a large number of datapoints, then the algorithm is forced to generalize and come up with a good model that suits most of the points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t have the luxury of gathering a large database all the time. Sometimes we are limited to a small database and we are forced to come up with a model based on that. In these situations, we use a technique called cross validation. What it does it that it splits the dataset into training and testing datasets. Only the datapoints in the training dataset are used to come up with the model and the testing dataset is used to test how good the model is. This is repeated with different partitions of training and testing datasets. This method gives a fairly good estimate of the underlying model because we are testing it on different partitions to generalize it as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) In the sense of model fitting, explain the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying learning curves. ...\n",
    "Using cross-validation correctly. ...\n",
    "Choosing the right error or score metric. ...\n",
    "Searching for the best hyper-parameters. ...\n",
    "Testing multiple models. ...\n",
    "Averaging models. ...\n",
    "Stacking models. ...\n",
    "Applying feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6:How would you rate an unsupervised learning model&#39;s success? What are the most common\n",
    "success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used evaluation metric for unsupervised machine learning is the Silhouette Coefficient and Dunns Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7:Is it possible to use a classification model for numerical data or a regression model for categorical\n",
    "data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple explanation for numerical variables if we normalize them then they will be between 0 and 1 if the value is near 1 that is greater than 0.5  and we can assign 0 if it's less than 0.5 so of we apply any classification model it will predict 0 Or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical variables also same logic if we convert them into using label encoding then there are numerical varibles if we apply regression it will give value in decimal values so we can round of the value to nearest integer and that is the label of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8:The following data were collected when using a classification model to predict the malignancy of a\n",
    "group of patients&#39; tumors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)Accurate estimates – 15 cancerous, 75 benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Wrong predictions – 3 cancerous, 7 benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the model&#39;s error rate, sensitivity, precision, and F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate = ((3+7)/100) * 100\n",
    "error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.33333333333334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = (15/(15+3)) * 100\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.18181818181817"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = (15/(15+7)) * 100\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.00000000000001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_score = ((precision*sensitivity)/(precision + sensitivity)) * 2\n",
    "F_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10. Make quick notes on:\n",
    "\n",
    "\n",
    "1. The process of holding out\n",
    "\n",
    "2. Cross-validation by tenfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Holdout Method is the simplest sort of method to evaluate a classifier. In this method, the data set (a collection of data items or examples) is separated into two sets, called the Training set and Test set. A classifier performs function of assigning data items in a given collection to a target category or clas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Cross- validation by tenfold means dividing the whole dataset into ten sets and using one set for testing and others for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10.Define the following terms:\n",
    "\n",
    "1) Boosting vs. Bagging\n",
    "\n",
    "2) The eager learner vs. the lazy learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Bagging is a method of merging the same type of predictions. Boosting is a method of merging different types of predictions.\n",
    "\n",
    "Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance.\n",
    "\n",
    "In Bagging, each model receives an equal weight. In Boosting, models are weighed based on their performance.\n",
    "\n",
    "Models are built independently in Bagging. New models are affected by a previously built model’s performance in Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bagging, training data subsets are drawn randomly with a replacement for the training dataset. In Boosting, every new subset comprises the elements that were misclassified by previous models.\n",
    "\n",
    "Bagging is usually applied where the classifier is unstable and has a high variance. Boosting is usually applied where the classifier is stable and simple and has high bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
